# ğŸ“ Laboratorio Big Data (Docker + SSD)

Este proyecto despliega un cluster de **Apache Spark** (Master + 2 Workers) utilizando Docker. EstÃ¡ diseÃ±ado para trabajar con grandes volÃºmenes de datos almacenados externamente en un disco SSD para no saturar tu ordenador.

## ğŸ“‹ Requisitos Previos

Antes de empezar, asegÃºrate de tener:
1.  **Docker Desktop** instalado y ejecutÃ¡ndose (con el Ã­cono de la ballena verde/blanco en la barra de tareas).
2.  Un **Disco SSD Externo** conectado (recomendado usar la letra **E:**).
3.  **PowerShell** (viene con Windows).

---

## ğŸ§  Â¿CÃ³mo funciona este Laboratorio?

Este proyecto no solo es un clÃºster de Spark, es un **Ecosistema de Datos Completo** que conecta tu hardware fÃ­sico (SSD) con herramientas de procesado masivo.

```mermaid
graph TD
    subgraph "1. Ingesta (Internet)"
        Web["ğŸŒ Football-Data.co.uk"] -- "Descarga" --> Prepare["ğŸ prepare_data.py"]
    end

    subgraph "2. Almacenamiento (Disco SSD)"
        Prepare -- "Mapeo" --> RealFolder["ğŸ“‚ BIGDATA_LAB_STORAGE/../data"]
        RealFolder -- "Consolida" --> CSV["ğŸ“„ laliga_history.csv"]
    end

    subgraph "3. Motores de Procesamiento (Docker)"
        CSV --> MR["ğŸ˜ Hadoop MapReduce (mrjob)"]
        CSV --> SP["ğŸš€ Spark SQL (PySpark)"]
        MR --> Report1["ğŸ“„ final_report.txt"]
        SP --> Report2["ğŸ“„ spark_reporte.txt"]
    end

    subgraph "4. PresentaciÃ³n (Web)"
        CSV --> Dash["ğŸ“Š Dashboard Interactivo (localhost:8081)"]
        Report1 -.-> Dash
        Report2 -.-> Dash
    end

    style RealFolder fill:#fff3e0,stroke:#e65100
    style Docker fill:#e8f5e9,stroke:#2e7d32
    style Dash fill:#e1f5fe,stroke:#01579b
```

### Â¿QuÃ© ocurre "bajo el capÃ³"?

1.  **Ingesta Inteligente**: El script `prepare_data.py` descarga automÃ¡ticamente las Ãºltimas 15 temporadas de La Liga, las limpia y las une en un Ãºnico archivo gigante en tu **SSD**.
2.  **Puente de Memoria**: Docker utiliza un archivo `.env` para saber exactamente dÃ³nde estÃ¡ tu SSD, montando los datos sin ocupar espacio en tu disco `C:`.
3.  **Procesamiento HÃ­brido**: 
    *   **ClÃ¡sico**: Usamos `mrjob` para emular el corazÃ³n de Hadoop (MapReduce) y calcular rÃ©cords histÃ³ricos.
    *   **Moderno**: Usamos `Spark SQL` para lanzar consultas "estilo base de datos" de alta velocidad sobre millones de registros.
4.  **VisualizaciÃ³n**: Todo se conecta a un servidor **Nginx** que sirve un Dashboard interactivo, donde puedes filtrar temporadas y ver los resultados en grÃ¡ficos dinÃ¡micos.

#### ğŸ”§ Detalles TÃ©cnicos Importantes

#### 1. Â¿Por quÃ© usamos un archivo `.env`?
Docker en Windows a veces tiene problemas para entender los "atajos" (Junctions). Para evitar errores como "file exists", el script genera automÃ¡ticamente un archivo oculto `.env`.
*   **Â¿QuÃ© hace?**: Guarda la **ruta real y exacta** de tu disco SSD (ej: `E:\BIGDATA...`).
*   **Beneficio**: El clÃºster de Spark ve el disco directamente, garantizando estabilidad total.

#### 2. Dashboard del Proyecto (Web)
Hemos incluido una pÃ¡gina web local (`localhost:8081`) para facilitar la presentaciÃ³n del proyecto.
*   **Contenedor**: Usa `Nginx`, un servidor web ultraligero.
*   **PropÃ³sito**: Mostrar visualmente la arquitectura y las evidencias de que el cluster estÃ¡ funcionando, ideal para explicÃ¡rselo a profesores o compaÃ±eros.

---

## ğŸš€ InstalaciÃ³n Paso a Paso

Si eres nuevo, sigue estos pasos exactos:

### 1. Preparar la Carpeta
Descarga los archivos del proyecto (`setup_ssd_bigdata_v3.ps1` y `docker-compose.yml`) en una carpeta, por ejemplo `Descargas` o una carpeta nueva llamada `MiLaboratorio`.

### 2. Ejecutar el Script de ConfiguraciÃ³n
Este script crea las conexiones "mÃ¡gicas" para que Docker pueda ver tu disco SSD como si fuera una carpeta local.

1.  Abre el menÃº Inicio, busca **PowerShell**, clic derecho y **"Ejecutar como administrador"**.
2.  Navega a la carpeta donde tienes los archivos. Por ejemplo:
    ```powershell
    cd C:\Users\alexi\Downloads
    ```
    *(Ajusta la ruta si los guardaste en otro sitio)*
3.  Ejecuta el script de instalaciÃ³n:
    ```powershell
    powershell -ExecutionPolicy Bypass -File .\setup_ssd_bigdata_v3.ps1
    ```
4.  El script te preguntarÃ¡ la letra de tu disco SSD. Si es **E**, solo presiona **Enter**.

### 3. Encender el Laboratorio
Una vez que el script termine (te dirÃ¡ "Listo"), ejecuta este comando para descargar y encender los servidores de Spark:

```powershell
docker compose up -d
```
*Esto puede tardar unos minutos la primera vez mientras descarga las herramientas.*

---

## âœ… Â¿CÃ³mo sÃ© si funciona?

1.  Abre tu navegador web favorito (Chrome, Edge, etc.).
2.  Entra a: [http://localhost:8080](http://localhost:8080)
3.  DeberÃ­as ver una pantalla que dice **"Spark Master at spark://..."**.
4.  Busca la lÃ­nea **"Alive Workers:"**. Si dice **2**, Â¡Felicidades! Todo funciona.

---

## ğŸ›‘ Apagar el Laboratorio

Cuando termines de trabajar y quieras liberar memoria RAM en tu PC, ejecuta:

```powershell
docker compose down
```

---

## ğŸ”„ Aplicar Cambios (Reiniciar Todo)

Si modificas el cÃ³digo (ej: cambios en `docker-compose.yml` o configuraciones), usa estos comandos para asegurarte de que Docker aplique los cambios limpiamente:

1.  **Detener y eliminar contenedores**:
    ```powershell
    docker compose down
    ```

2.  **Forzar recreaciÃ³n de contenedores**:
    ```powershell
    docker compose up -d --force-recreate
    ```
    *Esto obliga a Docker a reconstruir la configuraciÃ³n, Ãºtil si algo falla o cambiaste puertos/volÃºmenes.*

---

## ğŸ› ï¸ SoluciÃ³n de Problemas Comunes

*   **Error "execution of scripts is disabled..."**: AsegÃºrate de usar el comando largo del paso 2.3 que incluye `-ExecutionPolicy Bypass`.
*   **Error "docker not found"**: Docker Desktop no estÃ¡ abierto. BÃºscalo en Inicio y Ã¡brelo primero.
*   **Error con imagen "bitnami"**: Si Docker falla al descargar, revisa el archivo `docker-compose.yml` y asegÃºrate de que dice `image: apache/spark:3.5.0` (o `latest`).

---

**Â¡Disfruta aprendiendo Big Data!** ğŸ˜

---

## ğŸ“¸ Resultados y Evidencias

AquÃ­ estÃ¡ la prueba de que el laboratorio funciona correctamente:

### 1. Contenedores EjecutÃ¡ndose (Docker Desktop)
Se muestra el estado "Running" (verde) para todos los servicios.

![Docker Containers Running](./images/docker_success.png)

### 2. Spark Master UI
Accesible en `http://localhost:8080`, mostrando los 2 Workers registrados y listos.

![Spark Master UI](./images/spark_ui.png)

### 3. Dashboard del Proyecto
Accesible en `http://localhost:8081`. Muestra la arquitectura, el estado de los servicios y evidencia visual.

![Dashboard Web](./images/localhost-8081.png)

### 4. MÃ³dulo de Hadoop (AnalÃ­tica Avanzada)
Nuevo mÃ³dulo de anÃ¡lisis de datos histÃ³ricos (La Liga 2009-2024) usando MapReduce.

*   **Script de AnÃ¡lisis**: Ubicado en `hadoop_lab/run_analysis.ps1`. Descarga datos y genera estadÃ­sticas.
*   **Dashboard Interactivo**: Accesible en `http://localhost:8081/hadoop.html`.
    *   Permite filtrar por aÃ±os y ver grÃ¡ficas de Goles, Tarjetas y Victorias.

---

## ğŸ’» EjecuciÃ³n Manual (Alternativa)

Si la ejecuciÃ³n automÃ¡tica falla o prefieres hacerlo paso a paso, utiliza estos comandos en una terminal de **PowerShell como Administrador**.

âš ï¸ **Nota Importante sobre la Ruta:**
El comando `cd` de abajo es solo un **ejemplo/referencia**. Debes cambiarlo por la ruta real donde tÃº tengas guardada la carpeta `Prueba_Script_Docker` en tu PC.

```powershell
# 1. Ir a la carpeta de tu proyecto (Â¡AJUSTA ESTA RUTA A LA TUYA!)
cd "C:\Users\amendoza\.gemini\antigravity\scratch\Prueba_Script_Docker"

# 2. Ejecutar el script (ahora sÃ­ funcionarÃ¡ el enlace y .env)
# Si te sale error de "scripts disabled", usa este comando especial:
powershell -ExecutionPolicy Bypass -File .\setup_ssd_bigdata_v3.ps1

```powershell
# 3. Levantar Docker (si el script no lo hizo ya)
docker compose up -d
```

---

## ğŸ”„ CÃ³mo Actualizar (Desde GitHub)

Si en el futuro hago cambios en el repositorio y quieres bajarlos a tu PC:

### OpciÃ³n 1: ActualizaciÃ³n Normal
Usa esto si solo quieres traer lo nuevo:
```powershell
git pull origin main
```

### OpciÃ³n 2: Forzar SincronizaciÃ³n (Si algo falla)
Usa esto si tienes errores o quieres estar 100% igual que la nube (âš ï¸ Borra tus cambios locales no guardados):
```powershell
git fetch --all
git reset --hard origin/main
```

---

## ğŸ’¾ CÃ³mo Guardar tus Cambios (Subir a GitHub)

Cada vez que modifiques cÃ³digo o documentaciÃ³n y quieras guardarlo en la nube:

```powershell
git add .
git commit -m "Escribe aquÃ­ quÃ© cambiaste"
git push
```
*(Si es la primera vez en una sesiÃ³n nueva, GitHub podrÃ­a pedirte credenciales).*

---

## ğŸ“Š Resumen del MÃ³dulo Hadoop & Spark SQL

Como mejora final, se ha integrado un ecosistema completo de anÃ¡lisis de Big Data:

1.  **Hadoop MapReduce (Python)**: Procesamiento clÃ¡sico de goles, tarjetas y victorias usando `mrjob`.
2.  **Spark SQL (Docker)**: AnÃ¡lisis moderno de alto nivel con consultas SQL distribuidas en el clÃºster.
3.  **Dashboard Interactivo**: VisualizaciÃ³n profesional en tiempo real con filtros por temporada.
### ğŸ› ï¸ Comandos de EjecuciÃ³n
Si quieres lanzar los anÃ¡lisis manualmente desde la terminal:

*   **Hadoop (MapReduce)**:
    ```powershell
    powershell -File hadoop_lab/run_analysis.ps1
    ```
*   **Spark SQL (ClÃºster)**:
    ```powershell
    docker exec -it spark-master /opt/spark/bin/spark-submit /opt/spark/scripts/spark_analysis.py
    ```

> [!TIP]
> **Â¿Quieres saber mÃ¡s?** Si deseas ver el manual tÃ©cnico completo, el flujo de datos detallado y cÃ³mo ejecutar las consultas SQL avanzadas, consulta el archivo:
> ğŸ‘‰ **[Doc_Hadoop.md](./hadoop_lab/Doc_Hadoop.md)**

---

## ğŸš€ GuÃ­a RÃ¡pida (Para nuevas PCs o Clones)

Si acabas de clonar este proyecto o vas a usarlo en un ordenador nuevo, sigue estos 4 pasos crÃ­ticos:

1.  **Conecta tu SSD**: AsegÃºrate de que el disco estÃ© conectado (preferiblemente letra **E:**).
2.  **Configura el "Portal" (Junction)**: Ejecuta PowerShell como **Administrador** en la carpeta del proyecto y lanza:
    ```powershell
    powershell -ExecutionPolicy Bypass -File .\setup_ssd_bigdata_v3.ps1
    ```
3.  **Inicia el ClÃºster**:
    ```powershell
    docker compose up -d
    ```
4.  **Verifica y Disfruta**:
    *   Dashboard: [http://localhost:8081](http://localhost:8081)
    *   Spark Master: [http://localhost:8080](http://localhost:8080)

---

**Â¡Disfruta aprendiendo Big Data!** ğŸ˜ğŸš€
